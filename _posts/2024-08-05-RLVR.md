---
layout: post
title: "可验证奖励的强化学习（RLVR）"
date: 2024-08-05 11:00:00 +0800
categories: tech
---
    <style>
        :root {
            --primary: #2c1810;
            --secondary: #8b7355;
            --accent: #d4af37;
            --neutral: #f5f4f2;
            --base-100: #fefefe;
            --base-content: #1a1a1a;
        }
        
        body {
            font-family: 'Lora', serif;
            background: var(--neutral);
            color: var(--base-content);
            line-height: 1.7;
            overflow-x: hidden;
        }
        
        h1, h2, h3, h4, h5, h6 {
            font-family: 'Inter', sans-serif;
            font-weight: 600;
            color: var(--primary);
        }
        
        .toc-fixed {
            position: fixed;
            top: 0;
            left: 0;
            width: 280px;
            height: 100vh;
            background: var(--base-100);
            border-right: 1px solid #e5e5e5;
            z-index: 1000;
            overflow-y: auto;
            padding: 2rem 1.5rem;
            box-shadow: 2px 0 10px rgba(0,0,0,0.05);
        }
        
        .main-content {
            margin-left: 280px;
            min-height: 100vh;
        }
        
        .hero-grid {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 2rem;
            align-items: center;
            min-height: 60vh;
        }
        
        .hero-text {
            background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            word-break: break-word;
        }
        
        .toc-link {
            display: block;
            padding: 0.5rem 0;
            color: var(--secondary);
            text-decoration: none;
            border-left: 2px solid transparent;
            padding-left: 1rem;
            transition: all 0.3s ease;
            word-break: break-word;
        }
        
        .toc-link:hover, .toc-link.active {
            color: var(--primary);
            border-left-color: var(--accent);
            background: rgba(212, 175, 55, 0.1);
        }
        
        .toc-link.sub {
            font-size: 0.9rem;
            padding-left: 2rem;
            color: #666;
        }
        
        .section-card {
            background: var(--base-100);
            border-radius: 12px;
            padding: 2rem;
            margin: 2rem 0;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            border: 1px solid #f0f0f0;
        }
        
        .accent-border {
            border-left: 4px solid var(--accent);
            padding-left: 1.5rem;
        }
        
        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            margin: 1.5rem 0;
        }
        
        .citation {
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
            cursor: pointer;
        }
        
        .citation:hover {
            text-decoration: underline;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, rgba(212, 175, 55, 0.1) 0%, rgba(139, 115, 85, 0.1) 100%);
            border: 1px solid rgba(212, 175, 55, 0.3);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }
        
        .formula {
            background: #f8f9fa;
            padding: 1rem;
            border-radius: 6px;
            font-family: 'Courier New', monospace;
            margin: 1rem 0;
            border-left: 3px solid var(--accent);
        }
        
        @media (max-width: 768px) {
            .toc-fixed {
                transform: translateX(-100%);
                transition: transform 0.3s ease;
            }
            
            .toc-fixed.open {
                transform: translateX(0);
            }
            
            .main-content {
                margin-left: 0;
            }
            
            .hero-grid {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }
            
            #toc-toggle {
                display: block;
                position: fixed;
                top: 1rem;
                left: 1rem;
                z-index: 1100;
                background: var(--primary);
                color: white;
                border: none;
                border-radius: 4px;
                padding: 0.5rem 1rem;
                cursor: pointer;
            }
        }

        @media (min-width: 769px) {
            #toc-toggle {
                display: none;
            }
        }

        @media (max-width: 640px) {
            .hero-grid h1 {
                font-size: 2.25rem;
            }
            .hero-grid p {
                font-size: 1rem;
            }
            .hero-grid img {
                height: auto;
                max-height: 300px;
            }
        }
    </style>
    <!-- Table of Contents -->
    <nav class="toc-fixed">
      <button id="toc-toggle" class="md:hidden">目录</button>
      <div class="mb-8">
        <h3 class="text-lg font-bold text-primary mb-4">目录</h3>
        <a href="#overview" class="toc-link">概述</a>
        <a href="#core-concepts" class="toc-link">核心概念</a>
        <a href="#rlvr-definition" class="toc-link sub">RLVR定义</a>
        <a href="#rlhf-comparison" class="toc-link sub">RLVR vs RLHF</a>
        <a href="#workflow" class="toc-link">工作流程</a>
        <a href="#grpo-algorithm" class="toc-link">GRPO算法</a>
        <a href="#grpo-overview" class="toc-link sub">算法概述</a>
        <a href="#mathematical-principles" class="toc-link sub">数学原理</a>
        <a href="#code-implementation" class="toc-link">代码实现</a>
        <a href="#environment-setup" class="toc-link sub">环境准备</a>
        <a href="#core-code" class="toc-link sub">核心代码</a>
        <a href="#discussion" class="toc-link">讨论与展望</a>
      </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
      <!-- Hero Section -->
      <section class="relative bg-gradient-to-br from-neutral to-base-100 py-16 px-8">
        <div class="max-w-7xl mx-auto">
          <div class="hero-grid">
            <div class="space-y-6">
              <h1 class="text-5xl md:text-6xl font-bold leading-tight hero-text">
                <em>可验证奖励的强化学习</em>
              </h1>
              <p class="text-xl text-secondary font-medium">
                轻量化大语言模型的推理新范式
              </p>
              <div class="flex items-center space-x-4 text-sm text-gray-600">
                <span><i class="fas fa-calendar mr-2"></i>2025年6月</span>
                <span><i class="fas fa-bookmark mr-2"></i>技术报告</span>
              </div>
            </div>
            <div class="relative">
              <img src="https://kimi-web-img.moonshot.cn/img/newsroom.porsche.com/809c244e83f0b7b6faccbf91fcb8dc4fa258d536.jpeg" alt="神经网络抽象艺术图" class="w-full h-80 object-cover rounded-xl shadow-lg" size="medium" aspect="wide" query="神经网络抽象艺术" referrerpolicy="no-referrer" data-modified="1" data-score="0.00"/>
              <div class="absolute inset-0 bg-gradient-to-t from-black/20 to-transparent rounded-xl"></div>
            </div>
          </div>
        </div>
      </section>

      <!-- Key Highlights -->
      <section class="py-12 px-8 bg-base-100">
        <div class="max-w-6xl mx-auto">
          <div class="grid md:grid-cols-3 gap-8">
            <div class="highlight-box">
              <h3 class="text-xl font-bold mb-3 text-primary"><i class="fas fa-check-circle mr-2 text-accent"></i>客观验证</h3>
              <p class="text-gray-700">使用确定性验证器替代主观人类反馈，确保奖励信号的客观性和可重复性。</p>
            </div>
            <div class="highlight-box">
              <h3 class="text-xl font-bold mb-3 text-primary"><i class="fas fa-cogs mr-2 text-accent"></i>轻量化训练</h3>
              <p class="text-gray-700">GRPO算法无需独立Critic网络，显著降低计算开销，适合轻量化模型。</p>
            </div>
            <div class="highlight-box">
              <h3 class="text-xl font-bold mb-3 text-primary"><i class="fas fa-chart-line mr-2 text-accent"></i>推理优化</h3>
              <p class="text-gray-700">特别适用于数学、代码生成等需要精确推理的任务，提升模型逻辑能力。</p>
            </div>
          </div>
        </div>
      </section>

      <!-- Content Sections -->
      <div class="max-w-5xl mx-auto px-8 py-12">

        <!-- Overview -->
        <section id="overview" class="section-card">
          <h2 class="text-3xl font-bold mb-6">概述</h2>
          <div class="accent-border">
            <p class="text-lg leading-relaxed">
              可验证奖励的强化学习（RLVR）是一种专为提升大语言模型（LLM）在特定推理任务上表现而设计的训练范式。它通过引入一个客观、自动化的&#34;验证器&#34;来替代传统强化学习中依赖人类反馈或复杂奖励模型的评估机制。
            </p>
          </div>
          <p class="mt-6 text-gray-700">
            在RLVR框架中，LLM作为智能体，接收任务提示并生成响应，该响应随后被送入验证器进行检验。验证器根据预设的规则或程序，判断模型的输出是否正确，并返回一个明确的奖励信号。这种直接的、基于结果正确性的反馈机制，使得模型能够通过不断的试错学习，自主地优化其生成策略，从而更有效地解决具有明确答案的推理问题，如数学计算、代码生成和逻辑谜题等。
          </p>
        </section>

        <!-- Core Concepts -->
        <section id="core-concepts" class="section-card">
          <h2 class="text-3xl font-bold mb-6">RLVR核心概念与原理</h2>

          <div id="rlvr-definition" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">RLVR的定义与动机</h3>
            <p class="mb-4">
              可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）是一种新兴的、专为提升大语言模型（LLM）在逻辑推理和精确任务上表现而设计的训练范式。其核心思想在于，将强化学习（RL）的奖励信号直接与一个客观、可自动验证的正确性标准挂钩，而非依赖于主观的人类偏好或一个需要单独训练的奖励模型<a href="#ref-175" class="citation">[175]</a>。
            </p>

            <div class="highlight-box">
              <h4 class="font-semibold mb-2">核心优势</h4>
              <ul class="list-disc list-inside space-y-2 text-gray-700">
                <li>省去独立奖励模型的训练，降低计算开销</li>
                <li>避免奖励黑客（Reward Hacking）问题</li>
                <li>奖励信号直接与任务目标对齐</li>
                <li>特别适合轻量化模型的推理能力提升</li>
              </ul>
            </div>
          </div>

          <div id="rlhf-comparison" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">RLVR与RLHF的对比</h3>

            <div class="overflow-x-auto">
              <table class="w-full border-collapse border border-gray-300 rounded-lg overflow-hidden">
                <thead class="bg-gray-50">
                  <tr>
                    <th class="border border-gray-300 px-4 py-3 text-left font-semibold">特性</th>
                    <th class="border border-gray-300 px-4 py-3 text-left font-semibold">RLHF</th>
                    <th class="border border-gray-300 px-4 py-3 text-left font-semibold">RLVR</th>
                  </tr>
                </thead>
                <tbody class="bg-white">
                  <tr>
                    <td class="border border-gray-300 px-4 py-3 font-medium">核心目标</td>
                    <td class="border border-gray-300 px-4 py-3">对齐人类偏好，生成更受欢迎的回答</td>
                    <td class="border border-gray-300 px-4 py-3">提升客观正确性，解决具体问题</td>
                  </tr>
                  <tr class="bg-gray-50">
                    <td class="border border-gray-300 px-4 py-3 font-medium">奖励来源</td>
                    <td class="border border-gray-300 px-4 py-3">基于人类偏好数据训练的独立奖励模型</td>
                    <td class="border border-gray-300 px-4 py-3">基于任务内在规则的外部验证器</td>
                  </tr>
                  <tr>
                    <td class="border border-gray-300 px-4 py-3 font-medium">奖励性质</td>
                    <td class="border border-gray-300 px-4 py-3">连续、主观的分数</td>
                    <td class="border border-gray-300 px-4 py-3">二元（正确/错误）或基于明确规则</td>
                  </tr>
                  <tr class="bg-gray-50">
                    <td class="border border-gray-300 px-4 py-3 font-medium">适用场景</td>
                    <td class="border border-gray-300 px-4 py-3">开放域对话、创意写作等主观任务</td>
                    <td class="border border-gray-300 px-4 py-3">数学推理、代码生成等客观任务</td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        </section>

        <!-- Workflow -->
        <section id="workflow" class="section-card">
          <h2 class="text-3xl font-bold mb-6">RLVR的工作流程</h2>

          <p class="mb-6">
            RLVR的训练过程是一个迭代循环，通过不断的&#34;生成-验证-优化&#34;来逐步提升模型的推理能力。这个流程可以分解为四个关键步骤：
          </p>

          <div class="grid md:grid-cols-2 gap-6">
            <div class="bg-gradient-to-br from-blue-50 to-indigo-50 p-6 rounded-lg border border-blue-200">
              <h4 class="font-semibold text-lg mb-3 text-blue-800"><i class="fas fa-edit mr-2"></i>问题定义与提示工程</h4>
              <p class="text-blue-700">精心设计输入提示，包含系统指令和少样本示例，引导模型生成结构化输出。</p>
            </div>

            <div class="bg-gradient-to-br from-green-50 to-emerald-50 p-6 rounded-lg border border-green-200">
              <h4 class="font-semibold text-lg mb-3 text-green-800"><i class="fas fa-brain mr-2"></i>思维链生成</h4>
              <p class="text-green-700">模型生成包含详细推理过程的思维链，使用特定标签如
                <code>&lt;think&gt;</code>和
                <code>&lt;answer&gt;</code>来组织内容。
              </p>
            </div>

            <div class="bg-gradient-to-br from-purple-50 to-violet-50 p-6 rounded-lg border border-purple-200">
              <h4 class="font-semibold text-lg mb-3 text-purple-800"><i class="fas fa-check-double mr-2"></i>答案验证与奖励计算</h4>
              <p class="text-purple-700">从模型输出中提取答案，通过验证器进行客观检验，计算正确性奖励和格式奖励。</p>
            </div>

            <div class="bg-gradient-to-br from-orange-50 to-amber-50 p-6 rounded-lg border border-orange-200">
              <h4 class="font-semibold text-lg mb-3 text-orange-800"><i class="fas fa-cog mr-2"></i>策略优化与模型更新</h4>
              <p class="text-orange-700">使用GRPO等算法，基于奖励信号更新模型参数，优化生成策略。</p>
            </div>
          </div>
        </section>

        <!-- GRPO Algorithm -->
        <section id="grpo-algorithm" class="section-card">
          <h2 class="text-3xl font-bold mb-6">核心算法：组相对策略优化（GRPO）</h2>

          <div id="grpo-overview" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">GRPO算法概述</h3>
            <p class="mb-4">
              组相对策略优化（Group Relative Policy Optimization, GRPO）是一种专门为提升大语言模型（LLM）推理能力而设计的强化学习算法，它作为近端策略优化（PPO）的一种高效变体，在RLVR框架中扮演着核心角色<a href="#ref-143" class="citation">[143]</a>。
            </p>

            <div class="highlight-box">
              <h4 class="font-semibold mb-3">GRPO的核心创新</h4>
              <p class="mb-3">GRPO完全摒弃了独立的Critic网络，转而采用一种更为直接和高效的方式来估计优势：</p>
              <ul class="list-disc list-inside space-y-2 text-gray-700">
                <li>对于每个提示，生成一组（Group）多个响应</li>
                <li>计算组内奖励的均值和标准差</li>
                <li>通过&#34;白化&#34;（whitening）计算每个响应的相对优势</li>
                <li>优势 = (个体奖励 - 组平均奖励) / 组标准差</li>
              </ul>
            </div>
          </div>

          <div id="mathematical-principles" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">GRPO的数学原理</h3>

            <h4 class="text-lg font-semibold mb-3">组内相对优势计算</h4>
            <div class="formula">
              <p>对于一组响应 G，计算均值 μ 和标准差 σ：</p>
              <p>μ = (1/G) × Σ(r_i)</p>
              <p>σ = √[(1/G) × Σ(r_i - μ)²]</p>
              <p>每个响应的相对优势：A_i = (r_i - μ) / σ</p>
            </div>

            <h4 class="text-lg font-semibold mb-3 mt-6">损失函数设计</h4>
            <div class="formula">
              <p>L(θ) = E[min(ratio × A_i, clip(ratio, 1-ε, 1+ε) × A_i)] - β × D_KL(π_θ || π_ref)</p>
              <p>其中 ratio = π_θ(y_i|x) / π_θ_old(y_i|x)</p>
            </div>
          </div>
        </section>

        <!-- Code Implementation -->
        <section id="code-implementation" class="section-card">
          <h2 class="text-3xl font-bold mb-6">代码实战：使用RLVR微调轻量化模型</h2>

          <div id="environment-setup" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">环境准备与依赖安装</h3>

            <div class="code-block">
              <pre><code># 安装核心依赖库
pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.47.1
pip install trl
pip install peft
pip install datasets</code></pre>
            </div>

            <p class="mt-4 text-gray-700">
              这些库构成了RLVR训练的基础环境：
            </p>
            <ul class="list-disc list-inside space-y-1 text-gray-700 mt-2">
              <li><strong>PyTorch</strong>：深度学习框架</li>
              <li><strong>Transformers</strong>：加载预训练模型和分词器</li>
              <li><strong>TRL</strong>：Transformer Reinforcement Learning库，包含GRPOTrainer</li>
              <li><strong>PEFT</strong>：参数高效微调，支持LoRA等技术</li>
              <li><strong>Datasets</strong>：加载和处理数据集</li>
            </ul>
          </div>

          <div id="core-code" class="mb-8">
            <h3 class="text-2xl font-semibold mb-4">核心代码实现</h3>

            <h4 class="text-lg font-semibold mb-3">1. 加载模型与分词器</h4>
            <div class="code-block">
              <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 加载轻量化模型和分词器
model_name = &#34;gpt2&#34;  # 或 &#34;Qwen2.5-Math-1.5B&#34; 等
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 设置padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token</code></pre>
            </div>

            <h4 class="text-lg font-semibold mb-3 mt-6">2. 定义提示模板与答案提取</h4>
            <div class="code-block">
              <pre><code>def extract_answer(text):
    &#34;&#34;&#34;从模型输出中提取答案&#34;&#34;&#34;
    try:
        answer_part = text.split(&#34;&lt;answer&gt;&#34;)[1].split(&#34;&lt;/answer&gt;&#34;)[0]
        return answer_part.strip()
    except IndexError:
        return &#34;&#34;

# 定义系统提示
SYSTEM_PROMPT = &#34;Respond in the following format: &lt;reasoning&gt;...&lt;/reasoning&gt;&lt;answer&gt;...&lt;/answer&gt;&#34;</code></pre>
            </div>

            <h4 class="text-lg font-semibold mb-3 mt-6">3. 实现奖励函数</h4>
            <div class="code-block">
              <pre><code>def correctness_reward(completions, human_answers, **kwargs):
    &#34;&#34;&#34;正确性奖励函数&#34;&#34;&#34;
    rewards = []
    for completion, human_answer in zip(completions, human_answers):
        model_answer = extract_answer(completion)
        reward = 1.0 if model_answer == human_answer else -1.0
        rewards.append(reward)
    return rewards

def formatting_reward(completions, **kwargs):
    &#34;&#34;&#34;格式遵循奖励函数&#34;&#34;&#34;
    return [0.5 for _ in completions]  # 简化版本</code></pre>
            </div>

            <h4 class="text-lg font-semibold mb-3 mt-6">4. 配置LoRA与GRPO参数</h4>
            <div class="code-block">
              <pre><code>from peft import LoraConfig, get_peft_model, TaskType
from trl import GRPOConfig, GRPOTrainer

# 配置LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    task_type=&#34;CAUSAL_LM&#34;,
)

# 应用LoRA到模型
model = get_peft_model(model, lora_config)

# 配置GRPO训练参数
training_args = GRPOConfig(
    batch_size=4,
    llm_batch_size=4,
    num_epochs=5,
    learning_rate=3e-4,
)</code></pre>
            </div>

            <h4 class="text-lg font-semibold mb-3 mt-6">5. 加载数据并启动训练</h4>
            <div class="code-block">
              <pre><code>from datasets import load_dataset

# 加载GSM8K数据集
def get_gsm8k_questions(split=&#34;train&#34;):
    dataset = load_dataset(&#34;gsm8k&#34;, &#34;main&#34;, split=split)
    
    def extract_xml_answer(example):
        answer_text = example[&#34;answer&#34;].split(&#34;####&#34;)[-1].strip()
        return {
            &#34;prompt&#34;: [
                {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: SYSTEM_PROMPT},
                {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: example[&#34;question&#34;]}
            ],
            &#34;answer&#34;: answer_text
        }
    
    return dataset.map(extract_xml_answer)

train_dataset = get_gsm8k_questions(split=&#34;train&#34;)

# 初始化GRPOTrainer
trainer = GRPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
    peft_config=lora_config,
    correctness_reward=correctness_reward,
    formatting_reward=formatting_reward,
)

# 启动训练
trainer.train()

# 保存训练好的模型
trainer.save_model(&#34;rlvr_tuned_model&#34;)</code></pre>
            </div>
          </div>
        </section>

        <!-- Discussion -->
        <section id="discussion" class="section-card">
          <h2 class="text-3xl font-bold mb-6">讨论与展望</h2>

          <div class="grid md:grid-cols-2 gap-8">
            <div>
              <h3 class="text-xl font-semibold mb-4 text-red-700"><i class="fas fa-exclamation-triangle mr-2"></i>面临的挑战</h3>
              <ul class="space-y-3 text-gray-700">
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-red-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>对可验证任务的依赖</strong>
                    <br/>
                    仅适用于有明确正确答案的领域，难以扩展到主观性任务
                  </div>
                </li>
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-red-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>奖励函数设计复杂</strong>
                    <br/>
                    需要精心设计复合奖励函数，避免奖励黑客问题
                  </div>
                </li>
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-red-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>模型能力限制</strong>
                    <br/>
                    轻量化模型的基础能力限制了其在复杂任务上的表现
                  </div>
                </li>
              </ul>
            </div>

            <div>
              <h3 class="text-xl font-semibold mb-4 text-green-700"><i class="fas fa-lightbulb mr-2"></i>未来发展方向</h3>
              <ul class="space-y-3 text-gray-700">
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-green-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>算法创新</strong>
                    <br/>
                    结合更多强化学习算法，如基于模型的RL、多智能体RL
                  </div>
                </li>
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-green-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>领域扩展</strong>
                    <br/>
                    开发智能验证器，将RLVR扩展到更多主观任务领域
                  </div>
                </li>
                <li class="flex items-start">
                  <i class="fas fa-dot-circle text-green-500 mt-2 mr-3 text-xs"></i>
                  <div>
                    <strong>能力提升</strong>
                    <br/>
                    设计更精细的奖励函数，提升模型推理和泛化能力
                  </div>
                </li>
              </ul>
            </div>
          </div>

          <div class="mt-8 p-6 bg-gradient-to-r from-blue-50 to-indigo-50 rounded-lg border border-blue-200">
            <h4 class="text-lg font-semibold mb-3 text-blue-800">总结</h4>
            <p class="text-blue-700">
              RLVR作为一种新兴的训练范式，为大语言模型，特别是轻量化模型的推理能力提升提供了新的思路。通过结合客观验证和高效算法，它能够在资源受限的情况下，显著提升模型在特定任务上的表现。随着技术的不断发展，RLVR有望在更多领域发挥重要作用，推动AI技术的普惠化发展。
            </p>
          </div>
        </section>

      </div>

      <!-- References -->
      <section class="bg-gray-50 py-12 px-8">
        <div class="max-w-5xl mx-auto">
          <h2 class="text-2xl font-bold mb-6">参考文献</h2>
          <div class="grid gap-3 text-sm">
            <div id="ref-175" class="p-3 bg-white rounded border">
              <strong>[175]</strong> Reinforcement Learning with Verifiable Reward (RLVR): A New Paradigm for LLM Training - <a href="https://www.linkedin.com/pulse/reinforcement-learning-verifiable-reward-rlvr-new-paradigm-jatasra-xe3fc" class="citation">LinkedIn Article</a>
            </div>
            <div id="ref-143" class="p-3 bg-white rounded border">
              <strong>[143]</strong> DeepSeekMath Paper - <a href="https://arxiv.org/html/2503.06639v1" class="citation">arXiv:2503.06639</a>
            </div>
            <div id="ref-164" class="p-3 bg-white rounded border">
              <strong>[164]</strong> Awesome RLVR Collection - <a href="http://43.128.62.24:91/opendilab/awesome-RLVR" class="citation">OpenDILab Collection</a>
            </div>
            <div id="ref-172" class="p-3 bg-white rounded border">
              <strong>[172]</strong> Tulu v3 Paper Explanation - <a href="https://ritvik19.medium.com/papers-explained-183-tulu-v3-fc7758b18724" class="citation">Medium Article</a>
            </div>
            <div id="ref-13" class="p-3 bg-white rounded border">
              <strong>[13]</strong> How to Finetune Small Language Models to Think with Reinforcement Learning - <a href="https://towardsdatascience.com/how-to-finetune-small-language-models-to-think-with-reinforcement-learning/" class="citation">Towards Data Science</a>
            </div>
          </div>
        </div>
      </section>
    </main>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('.toc-link').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const targetElement = document.getElementById(targetId);
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
                // Close TOC on mobile after clicking a link
                if (window.innerWidth <= 768) {
                    document.querySelector('.toc-fixed').classList.remove('open');
                }
            });
        });

        // Active navigation highlighting
        window.addEventListener('scroll', function() {
            const sections = document.querySelectorAll('section[id]');
            const scrollPos = window.scrollY + 100;
            
            sections.forEach(section => {
                const top = section.offsetTop;
                const bottom = top + section.offsetHeight;
                const id = section.getAttribute('id');
                
                if (scrollPos >= top && scrollPos <= bottom) {
                    document.querySelectorAll('.toc-link').forEach(link => {
                        link.classList.remove('active');
                    });
                    const activeLink = document.querySelector(`a[href="#${id}"]`);
                    if (activeLink) {
                        activeLink.classList.add('active');
                    }
                }
            });
        });

        // Citation hover effects
        document.querySelectorAll('.citation').forEach(citation => {
            citation.addEventListener('click', function(e) {
                e.preventDefault();
                const refId = this.getAttribute('href');
                const refElement = document.querySelector(refId);
                if (refElement) {
                    refElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'center'
                    });
                    refElement.style.backgroundColor = '#fef3c7';
                    setTimeout(() => {
                        refElement.style.backgroundColor = '';
                    }, 2000);
                }
            });
        });

        // Toggle TOC on mobile
        document.getElementById('toc-toggle').addEventListener('click', function() {
            const toc = document.querySelector('.toc-fixed');
            toc.classList.toggle('open');
        });

        // Close TOC when clicking outside on mobile
        document.addEventListener('click', function(event) {
            const toc = document.querySelector('.toc-fixed');
            const tocToggle = document.getElementById('toc-toggle');
            
            if (window.innerWidth <= 768 && 
                !toc.contains(event.target) && 
                event.target !== tocToggle &&
                toc.classList.contains('open')) {
                toc.classList.remove('open');
            }
        });
    </script>
